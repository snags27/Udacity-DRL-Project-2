{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity DRL Project 2: Continuous Control\n",
    "\n",
    "---\n",
    "This notebook provides an outline of a solution to meet the task objectives of the Udacity Deep Reinforcement Learning Nanodegree Project 2. The task set was to solve the 'Reacher' Unity Environment which is an environment with a double jointed arm that is rewarded at each timestep that the agent's 'hand' is within the goal location. The environment is considered solved once the average score across 100 episodes is >= 30.\n",
    "\n",
    "For this project I have solved the distributed training (20 identical agents) version of the environment.\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment, which has 20 identical agents in order to distribute the task of gathering experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Windows_x86_64_20/Reacher.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unity environment contains **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview of Environment\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "The code below provides information on the number of agents as well as the action and state space sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "#print(len(env_info.ps))\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overview of Solution\n",
    "\n",
    "### 3.1 Algorithm\n",
    "\n",
    "The actor critic algorithm selected to complete this task was Proximal Policy Optimization (PPO) with a clipped objective. This is an on-policy algorithm. In addition Generalized Advantage Estimation (GAE) has been imlemented. The implementation of the alogrithm has been based a PPO implementation by Phil Tabor (https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/PPO/torch) but has been adapted for both a continuous environment as well as the use of multiple agents.\n",
    "\n",
    "\n",
    "### 3.2 Network Design\n",
    "\n",
    "Separate networks have been produced for both the actor and critic, with no shared elements. This architecture showed similar or better performance than some other alternatives with a shared input layer(s). An overview of the architecture is given below. The actor policy generates actions stochastically from a normal distribution.\n",
    "\n",
    "Actor\n",
    "\n",
    "    s -> Linear(s, 256) -> ReLU -> Linear(256, 256) -> ReLU -> Linear(256, a) -> Tanh -> Normal Distribution,\n",
    "        where s and a is the state and action space sizes respectively.\n",
    "\n",
    "Critic\n",
    "\n",
    "    s -> Linear(s, 256) -> ReLU -> Linear(256, 256) -> ReLU -> Linear(256, 1) -> V(s)\n",
    "\n",
    "The logic for the actor and critic networks is implemented as class `ActorNetwork` and `CriticNetwork` within `ppo.py`.\n",
    "\n",
    "### 3.3 Hyperparameters\n",
    "\n",
    "Agent hyperparameters are passed to the constructor class `Agent` in `ppo.py`. The hyperparameter values have been selected by a limited optimization process of varying the values and determining the best values for stability and performance.\n",
    "\n",
    "| parameter                | value      | description                                                                   |\n",
    "|--------------------------|------------|-------------------------------------------------------------------------------|\n",
    "| optimizer_learning_rate  | 1e-3       | Learning rate for Adam optimizer                                              |\n",
    "| gamma                    | 0.99       | Discount rate for future rewards                                              |\n",
    "| gae_lambda               | 0.9        | Smoothing parameter for GAE                                                   |\n",
    "| n_epochs                 | 4          | Number of optimization steps to perform after trajectory rollback             |\n",
    "| batch_size               | 256        | Number of N-agent experiences to collect for a single optimization step       |\n",
    "| policy_clip              | 0.2        | Clipping parameter for the policy loss function                               |\n",
    "| value_loss_weight        | 1.0        | Weight applied to value loss on total loss function                           |\n",
    "\n",
    "The training hyperparameters defined below are passed to the training function, `train_agent`, defined within this workbook. \n",
    "\n",
    "| parameter                     | value     | description                                           |\n",
    "|-------------------------------|-----------|-------------------------------------------------------|\n",
    "| n_episodes                    | 300       | Maximum number of training episodes                   |\n",
    "| max_t                         | 1000      | Trajectory length, maximum no. of timesteps           |\n",
    "| N                             | 1000      | Number of timesteps before performing a policy update |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO actor critic model\n",
    "from ppo import Agent\n",
    "\n",
    "#Set agent hyperparameters\n",
    "alpha = 1e-3\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.9\n",
    "n_epochs=4\n",
    "batch_size=256\n",
    "policy_clip = 0.2\n",
    "value_loss_weight = 1.0\n",
    "\n",
    "# Initialize agent\n",
    "# Agent(n_actions, input_dims, gamma, alpha, gae_lambda, policy_clip, batch_size, n_epochs, value_loss_weight)\n",
    "agent = Agent(action_size, state_size, gamma, alpha, gae_lambda, policy_clip, batch_size, n_epochs, value_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def train_agent_(env, agent, n_episodes=300, max_t=1000, \n",
    "    display_every=10, solved_score=30, N=1):\n",
    "    \n",
    "    st_time = time.time()\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    n_steps = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):    \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        n_actors = len(env_info.vector_observations)\n",
    "        score = np.zeros(n_actors)\n",
    "        episode_length_list = []\n",
    "        \n",
    "        \n",
    "        for t in range(1, max_t+1):\n",
    "            states = env_info.vector_observations\n",
    "\n",
    "            actions, probs, vals = agent.choose_action(states)\n",
    "\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            next_states = env_info.vector_observations\n",
    "            \n",
    "            rewards = np.array(env_info.rewards)       \n",
    "            \n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            n_steps += 1\n",
    "            \n",
    "            score += np.array(rewards)\n",
    "            score = np.where(dones, 0, score)\n",
    "\n",
    "            # Collect experience at timestep\n",
    "            agent.remember(states, actions, probs, vals, rewards, dones)\n",
    "            \n",
    "            # If N timesteps have elapsed, use collected experiences to train agent\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "            \n",
    "            states = next_states\n",
    "\n",
    "        mean_score = score.mean()\n",
    "        scores_window.append(mean_score)       # save most recent score\n",
    "        scores.append(mean_score)              # save most recent score\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage score: {:.2f}'.format(\n",
    "            i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % display_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage score: {:.2f}'.format(\n",
    "                i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window) >= solved_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage score: {:.2f}'.format(\n",
    "                np.maximum(i_episode, 0), np.mean(scores_window)))\n",
    "\n",
    "            agent.save_models\n",
    "                \n",
    "            break\n",
    "    \n",
    "    print('\\nRun Time: {:.2f}s'.format(time.time() - st_time))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage score: 0.59\n",
      "Episode 20\tAverage score: 0.60\n",
      "Episode 30\tAverage score: 0.45\n",
      "Episode 40\tAverage score: 0.36\n",
      "Episode 50\tAverage score: 0.32\n",
      "Episode 60\tAverage score: 0.30\n",
      "Episode 70\tAverage score: 0.33\n",
      "Episode 80\tAverage score: 0.38\n",
      "Episode 90\tAverage score: 0.45\n",
      "Episode 100\tAverage score: 0.56\n",
      "Episode 110\tAverage score: 0.77\n",
      "Episode 120\tAverage score: 1.08\n",
      "Episode 130\tAverage score: 1.53\n",
      "Episode 140\tAverage score: 2.34\n",
      "Episode 150\tAverage score: 3.65\n",
      "Episode 160\tAverage score: 5.27\n",
      "Episode 170\tAverage score: 7.16\n",
      "Episode 180\tAverage score: 9.33\n",
      "Episode 190\tAverage score: 11.59\n",
      "Episode 200\tAverage score: 13.96\n",
      "Episode 210\tAverage score: 16.39\n",
      "Episode 220\tAverage score: 18.87\n",
      "Episode 230\tAverage score: 21.63\n",
      "Episode 240\tAverage score: 24.30\n",
      "Episode 250\tAverage score: 26.48\n",
      "Episode 260\tAverage score: 28.55\n",
      "Episode 268\tAverage score: 30.07\n",
      "Environment solved in 268 episodes!\tAverage score: 30.07\n",
      "\n",
      "Run Time: 831.47s\n"
     ]
    }
   ],
   "source": [
    "scores = train_agent_(env, agent, n_episodes=300, solved_score = 30, N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode No.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the trained agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score this episode: 38.78149913316592\n"
     ]
    }
   ],
   "source": [
    "# load the checkpoint file\n",
    "agent.load_models\n",
    "\n",
    "# Run through once with loaded model\n",
    "env_info = env.reset(train_mode=False)[brain_name]     \n",
    "states = env_info.vector_observations                  \n",
    "scores = np.zeros(num_agents)                          \n",
    "while True:\n",
    "    actions, _, _ = agent.choose_action(states)                       \n",
    "    env_info = env.step(actions)[brain_name]           \n",
    "    next_states = env_info.vector_observations         \n",
    "    rewards = env_info.rewards                         \n",
    "    dones = env_info.local_done                        \n",
    "    scores += env_info.rewards                         \n",
    "    states = next_states                               \n",
    "    if np.any(dones):                                  \n",
    "        break\n",
    "print('Total score this episode: {:.2f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results & Conclusions & Future Work\n",
    "\n",
    "The selected algorithm was successful in solving the environment within the requirements with a fast timeframe (~10 mins on high end consumer PC with a RTX2080 Ti). The PPO algorithm demonstrated a high level of robustness whilst varying the hyperparameters and the network architectures, with the documented hyperparameters giving the best performance.\n",
    "\n",
    "Some ideas for future work include:\n",
    "- Adapting the algorithm to other environments to gauge performance.\n",
    "- Performance comparison to other algorithms, for example the DDPG benchmark solved in approximately half the number of timesteps.\n",
    "- Review of further improvements to PPO algorithm to improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
